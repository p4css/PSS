{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb591a4b",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6defb",
   "metadata": {},
   "source": [
    "## Reading data -> Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b386d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/61/5bvzqdmn7455dm96br7vs9jw0000gn/T/jieba.cache\n",
      "Loading model cost 0.551 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>店家很給力，快遞也是相當快，第三次光顧啦</td>\n",
       "      <td>[店家, 很, 給力, ，, 快遞, 也, 是, 相當快, ，, 第三次, 光顧, 啦]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>這樣的配置用Vista系統還是有點卡。 指紋收集器。 沒送原裝滑鼠還需要自己買，不太好。</td>\n",
       "      <td>[這樣, 的, 配置, 用, Vista, 系統, 還是, 有點, 卡, 。,  , 指紋,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P</td>\n",
       "      <td>不錯，在同等檔次酒店中應該是值得推薦的！</td>\n",
       "      <td>[不錯, ，, 在, 同等, 檔次, 酒店, 中應, 該, 是, 值得, 推薦, 的, ！]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>哎！ 不會是蒙牛乾的吧 嚴懲真凶！</td>\n",
       "      <td>[哎, ！,  , 不會, 是, 蒙牛, 乾, 的, 吧,  , 嚴懲, 真凶, ！]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N</td>\n",
       "      <td>空尤其是三立電視臺女主播做的序尤其無趣像是硬湊那麼多字</td>\n",
       "      <td>[空, 尤其, 是, 三立, 電視, 臺, 女主播, 做, 的, 序, 尤其, 無趣, 像是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>P</td>\n",
       "      <td>價效比高、記憶體大、功能全，螢幕超清晰</td>\n",
       "      <td>[價效, 比高, 、, 記憶體, 大, 、, 功能, 全, ，, 螢幕超, 清晰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6384</th>\n",
       "      <td>N</td>\n",
       "      <td>你太狠了… 告訴你他們不會喧譁的人，肯定是蒙牛喝多了</td>\n",
       "      <td>[你, 太狠, 了, …,  , 告訴, 你, 他們, 不會, 喧, 譁, 的, 人, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6385</th>\n",
       "      <td>N</td>\n",
       "      <td>醫生居然買了蒙牛，我是喝呢還是不喝呢還是不喝呢？</td>\n",
       "      <td>[ , 醫生, 居然, 買, 了, 蒙牛, ，, 我, 是, 喝, 呢, 還是, 不, 喝,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6386</th>\n",
       "      <td>N</td>\n",
       "      <td>我只想說 夾蒙牛是不對的 販賣毒品是犯罪行為</td>\n",
       "      <td>[我, 只, 想, 說,  , 夾, 蒙牛, 是, 不, 對, 的,  , 販賣, 毒品, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6387</th>\n",
       "      <td>P</td>\n",
       "      <td>蒙牛便宜</td>\n",
       "      <td>[蒙牛, 便宜]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6388 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tag                                          text  \\\n",
       "0      P                          店家很給力，快遞也是相當快，第三次光顧啦   \n",
       "1      N  這樣的配置用Vista系統還是有點卡。 指紋收集器。 沒送原裝滑鼠還需要自己買，不太好。   \n",
       "2      P                          不錯，在同等檔次酒店中應該是值得推薦的！   \n",
       "3      N                             哎！ 不會是蒙牛乾的吧 嚴懲真凶！   \n",
       "4      N                   空尤其是三立電視臺女主播做的序尤其無趣像是硬湊那麼多字   \n",
       "...   ..                                           ...   \n",
       "6383   P                           價效比高、記憶體大、功能全，螢幕超清晰   \n",
       "6384   N                    你太狠了… 告訴你他們不會喧譁的人，肯定是蒙牛喝多了   \n",
       "6385   N                      醫生居然買了蒙牛，我是喝呢還是不喝呢還是不喝呢？   \n",
       "6386   N                        我只想說 夾蒙牛是不對的 販賣毒品是犯罪行為   \n",
       "6387   P                                          蒙牛便宜   \n",
       "\n",
       "                                             token_text  \n",
       "0          [店家, 很, 給力, ，, 快遞, 也, 是, 相當快, ，, 第三次, 光顧, 啦]  \n",
       "1     [這樣, 的, 配置, 用, Vista, 系統, 還是, 有點, 卡, 。,  , 指紋,...  \n",
       "2        [不錯, ，, 在, 同等, 檔次, 酒店, 中應, 該, 是, 值得, 推薦, 的, ！]  \n",
       "3           [哎, ！,  , 不會, 是, 蒙牛, 乾, 的, 吧,  , 嚴懲, 真凶, ！]  \n",
       "4     [空, 尤其, 是, 三立, 電視, 臺, 女主播, 做, 的, 序, 尤其, 無趣, 像是...  \n",
       "...                                                 ...  \n",
       "6383          [價效, 比高, 、, 記憶體, 大, 、, 功能, 全, ，, 螢幕超, 清晰]  \n",
       "6384  [你, 太狠, 了, …,  , 告訴, 你, 他們, 不會, 喧, 譁, 的, 人, ，,...  \n",
       "6385  [ , 醫生, 居然, 買, 了, 蒙牛, ，, 我, 是, 喝, 呢, 還是, 不, 喝,...  \n",
       "6386  [我, 只, 想, 說,  , 夾, 蒙牛, 是, 不, 對, 的,  , 販賣, 毒品, ...  \n",
       "6387                                           [蒙牛, 便宜]  \n",
       "\n",
       "[6388 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/sentiment.csv')\n",
    "\n",
    "import jieba\n",
    "df['token_text'] = df['text'].apply(lambda x:list(jieba.cut(x)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e949b5",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797ca7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata # for removing Chinese puctuation\n",
    "def remove_punc_by_unicode(words):\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if word != \" \" and not unicodedata.category(word[0]).startswith('P'):\n",
    "            out.append(word)\n",
    "    return out\n",
    "df['cleaned'] = df['token_text'].apply(remove_punc_by_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3365f",
   "metadata": {},
   "source": [
    "## tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c1bae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['店家 很 給力 快遞 也 是 相當快 第三次 光顧 啦',\n",
       " '這樣 的 配置 用 Vista 系統 還是 有點 卡 指紋 收集器 沒送 原裝 滑鼠 還 需要 自己 買 不太好',\n",
       " '不錯 在 同等 檔次 酒店 中應 該 是 值得 推薦 的',\n",
       " '哎 不會 是 蒙牛 乾 的 吧 嚴懲 真凶',\n",
       " '空 尤其 是 三立 電視 臺 女主播 做 的 序 尤其 無趣 像是 硬 湊 那麼 多字']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\" \".join(doc) for doc in df['cleaned']]\n",
    "y = df.iloc[:, 0]\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4f5ca",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e3e22",
   "metadata": {},
   "source": [
    "## Vectorization (not neccesary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282a8bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6388, 12240)\n",
      "Frequency of 推薦:  5830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_vect = count_vect.fit_transform(documents)\n",
    "print(X_vect.shape)\n",
    "print(\"Frequency of 推薦: \", count_vect.vocabulary_.get(u'推薦'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfdfe8",
   "metadata": {},
   "source": [
    "## tfidf vectorization\n",
    "- https://towardsdatascience.com/clustering-documents-with-python-97314ad6a78d\n",
    "- https://blog.csdn.net/blmoistawinde/article/details/80816179\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "**Parameters**\n",
    "- **lowercasebool**, default=True: Convert all characters to lowercase before tokenizing.\n",
    "- **analyzer{‘word’, ‘char’, ‘char_wb’}** or callable, default=’word’ Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "- **stop_words{‘english’}**, list, default=None\n",
    "- **token_pattern**, str, default=r”(?u)\\b\\w\\w+\\b”: the default setting limits on at least 2 characters\n",
    "- **ngram_range**, tuple (min_n, max_n), default=(1, 1): (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "- **max_df(min_df)** float or int, default=1.0: When building the vocabulary ignore terms that have a document frequency strictly higher(lower for min_df) than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "- **max_features**, int, default=None: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "- **use_idf**, default=True: Enable inverse-document-frequency reweighting.\n",
    "- **smooth_idf**, default=True: Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "976f176b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(6388, 11959)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['給力', '第三次', '相當快', '快遞', '店家', '光顧'], dtype='<U24'),\n",
       " array(['需要', '配置', '系統', '滑鼠', '沒送', '有點', '收集器', '指紋', '原裝', '不太好',\n",
       "        'vista'], dtype='<U24'),\n",
       " array(['檔次', '推薦', '同等', '值得', '中應'], dtype='<U24'),\n",
       " array(['真凶', '嚴懲'], dtype='<U24'),\n",
       " array(['電視', '無趣', '尤其', '女主播', '多字', '像是', '三立'], dtype='<U24'),\n",
       " array(['本書', '明明', '只到', '原因', '信的過', '以後怎麼'], dtype='<U24'),\n",
       " array(['感覺還', '一下'], dtype='<U24'),\n",
       " array(['顯示', '還不錯', '速度', '硬碟', '玩遊戲', '溫度', '散熱', '以下', 'cpu', '56'],\n",
       "       dtype='<U24'),\n",
       " array(['配置', '還不錯', '速度', '貼紙', '白色', '方便', '好看', '外觀', '執行', '主流',\n",
       "        'vista'], dtype='<U24'),\n",
       " array(['重新', '還要', '花灑', '水超級', '水壓', '標配', '本來', '換一個', '問題', '售後還',\n",
       "        '修理', '一下'], dtype='<U24')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/stopwords_zh-tw.txt\", encoding=\"utf-8\") as fin:\n",
    "    stopwords = fin.read().split(\"\\n\")[1:]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_df=0.05,\n",
    "                              # token_pattern=r\"(?u)\\b\\w+\\b\", # default=r”(?u)\\b\\w\\w+\\b” means at least 2 characters\n",
    "                              # max_features = 2000,\n",
    "                              stop_words=stopwords).fit(documents)\n",
    "\n",
    "X_tfidf = tfidf.transform(documents)\n",
    "print(type(X_tfidf))\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "\n",
    "# Show transform(X_tfidf) result\n",
    "tfidf.inverse_transform(X_tfidf)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b389e2b",
   "metadata": {},
   "source": [
    "## w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "757c0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "model = Doc2Vec(tagged_documents, vector_size=100, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08100abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6388, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "doc_vector = [model.infer_vector(doc) for doc in df['cleaned']]\n",
    "X_w2v = scipy.sparse.csr_matrix(doc_vector)\n",
    "X_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6377c",
   "metadata": {},
   "source": [
    "## with chi.square feature selector\n",
    "- https://towardsdatascience.com/using-the-chi-squared-test-for-feature-selection-with-implementation-b15a4dad93f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c6ccfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15789817, 0.01763186, 0.45339518, ..., 0.32014909, 0.00125132,\n",
       "       0.33394599])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(score_func=chi2, k=2000)\n",
    "y = df.iloc[:, 0]\n",
    "fit = selector.fit(X_tfidf, y)\n",
    "fit.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce8cd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6388x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25329 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_chi2 = selector.fit_transform(X_tfidf, y)\n",
    "X_chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110f191",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- Pipeline只是幫忙設計整個 Vectorization -> Feature selection -> Training -> Evaluating 的流程。所以必須要自己做`train_test_split()`。但若用了`GridSearchCV`，那Grid會自己跑nfold，看要設計多少個幾個folds，所以不用特地做trian-test-split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00de48",
   "metadata": {},
   "source": [
    "## train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a904fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78641bd",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "- `LogisticRegression()` https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5f161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/stopwords_zh-tw.txt\", encoding=\"utf-8\") as fin:\n",
    "    stopwords = fin.read().split(\"\\n\")[1:]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_df=0.05, stop_words=stopwords)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d4e23",
   "metadata": {},
   "source": [
    "## Using pipeline without Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a31aaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.05,\n",
       "                                 stop_words=['?', '、', '。', '“', '”', '《', '》',\n",
       "                                             '！', '，', '：', '；', '？', '人民',\n",
       "                                             '末##末', '啊', '阿', '哎', '哎呀', '哎喲',\n",
       "                                             '唉', '我', '我們', '按', '按照', '依照',\n",
       "                                             '吧', '吧噠', '把', '罷了', '被', ...])),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1bd0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8142931664058425\n",
      "[[867 131]\n",
      " [225 694]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.79      0.87      0.83       998\n",
      "           P       0.84      0.76      0.80       919\n",
      "\n",
      "    accuracy                           0.81      1917\n",
      "   macro avg       0.82      0.81      0.81      1917\n",
      "weighted avg       0.82      0.81      0.81      1917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cm = confusion_matrix(y_test, predicted)\n",
    "cr = classification_report(y_test, predicted)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(accuracy)\n",
    "print(cm)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f323d",
   "metadata": {},
   "source": [
    "# GridSearchCV + Pipeline\n",
    "\n",
    "Notes. 在Pipeline的寫法中，不能把不同的models passthrough到parameters才給，必須要在Pipeline時就要給estimator（也就是ML models）。如果要這麼做的話，就是寫for-loop。例如第五節。\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- Selecting dimensionality reduction with Pipeline and GridSearchCV https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "- https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66009c",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec4bfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/stopwords_zh-tw.txt\", encoding=\"utf-8\") as fin:\n",
    "    stopwords = fin.read().split(\"\\n\")[1:]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094baa6",
   "metadata": {},
   "source": [
    "## Designing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46670aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe= Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e59e1",
   "metadata": {},
   "source": [
    "## Designing parameters for GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34226d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_df': [0.01, 0.05, 0.1, 0.2, 1.0],\n",
    "#     'tfidf__token_pattern': [r\"(?u)\\b\\w+\\b\", r\"(?u)\\b\\w\\w+\\b\"],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "#     'clf__penalty': ('l1', 'l2', 'none'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64533d8",
   "metadata": {},
   "source": [
    "## Initialize Grid\n",
    "- `cv` for cross-validation folds, which means that the GridSearchCV will splitting data into 5 folds as training-testing data automatically. No need to split data into train and test manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a0180be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words=['?', '、',\n",
       "                                                                    '。', '“',\n",
       "                                                                    '”', '《',\n",
       "                                                                    '》', '！',\n",
       "                                                                    '，', '：',\n",
       "                                                                    '；', '？',\n",
       "                                                                    '人民',\n",
       "                                                                    '末##末', '啊',\n",
       "                                                                    '阿', '哎',\n",
       "                                                                    '哎呀', '哎喲',\n",
       "                                                                    '唉', '我',\n",
       "                                                                    '我們', '按',\n",
       "                                                                    '按照', '依照',\n",
       "                                                                    '吧', '吧噠',\n",
       "                                                                    '把', '罷了',\n",
       "                                                                    '被', ...])),\n",
       "                                       ('clf', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tfidf__max_df': [0.01, 0.05, 0.1, 0.2, 1.0],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tfidf__use_idf': (True, False)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose = 3)\n",
    "grid.fit(documents, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80dd6a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8498727333110295\n",
      "Params: \n",
      "tfidf__max_df: 1.0\n",
      "tfidf__ngram_range: (1, 1)\n",
      "tfidf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(\"Params: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13fd733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_tfidf__max_df</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.822379</td>\n",
       "      <td>0.805947</td>\n",
       "      <td>0.805164</td>\n",
       "      <td>0.779953</td>\n",
       "      <td>0.810493</td>\n",
       "      <td>0.804787</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.819249</td>\n",
       "      <td>0.805164</td>\n",
       "      <td>0.794210</td>\n",
       "      <td>0.768990</td>\n",
       "      <td>0.796398</td>\n",
       "      <td>0.796802</td>\n",
       "      <td>0.016457</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.840376</td>\n",
       "      <td>0.830203</td>\n",
       "      <td>0.837246</td>\n",
       "      <td>0.797181</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.830461</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.838811</td>\n",
       "      <td>0.820814</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>0.790916</td>\n",
       "      <td>0.829287</td>\n",
       "      <td>0.820754</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.837246</td>\n",
       "      <td>0.838028</td>\n",
       "      <td>0.834898</td>\n",
       "      <td>0.818324</td>\n",
       "      <td>0.835552</td>\n",
       "      <td>0.832810</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.831768</td>\n",
       "      <td>0.827074</td>\n",
       "      <td>0.826291</td>\n",
       "      <td>0.810493</td>\n",
       "      <td>0.824589</td>\n",
       "      <td>0.824043</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.05</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.837246</td>\n",
       "      <td>0.843505</td>\n",
       "      <td>0.832551</td>\n",
       "      <td>0.819890</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.836098</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.05</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.825509</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.820814</td>\n",
       "      <td>0.811276</td>\n",
       "      <td>0.830854</td>\n",
       "      <td>0.824357</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.836463</td>\n",
       "      <td>0.836463</td>\n",
       "      <td>0.834898</td>\n",
       "      <td>0.820673</td>\n",
       "      <td>0.839468</td>\n",
       "      <td>0.833593</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.827074</td>\n",
       "      <td>0.825509</td>\n",
       "      <td>0.811276</td>\n",
       "      <td>0.821457</td>\n",
       "      <td>0.823730</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.840376</td>\n",
       "      <td>0.840376</td>\n",
       "      <td>0.830203</td>\n",
       "      <td>0.818324</td>\n",
       "      <td>0.844166</td>\n",
       "      <td>0.834689</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.826291</td>\n",
       "      <td>0.831768</td>\n",
       "      <td>0.824726</td>\n",
       "      <td>0.810493</td>\n",
       "      <td>0.825372</td>\n",
       "      <td>0.823730</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.851330</td>\n",
       "      <td>0.850548</td>\n",
       "      <td>0.837118</td>\n",
       "      <td>0.851214</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.841158</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.844288</td>\n",
       "      <td>0.830854</td>\n",
       "      <td>0.840251</td>\n",
       "      <td>0.840950</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.852113</td>\n",
       "      <td>0.848983</td>\n",
       "      <td>0.845853</td>\n",
       "      <td>0.833986</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.845647</td>\n",
       "      <td>0.006192</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.843505</td>\n",
       "      <td>0.844288</td>\n",
       "      <td>0.832551</td>\n",
       "      <td>0.825372</td>\n",
       "      <td>0.840251</td>\n",
       "      <td>0.837193</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.856025</td>\n",
       "      <td>0.852895</td>\n",
       "      <td>0.853678</td>\n",
       "      <td>0.835552</td>\n",
       "      <td>0.851214</td>\n",
       "      <td>0.849873</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.841941</td>\n",
       "      <td>0.830070</td>\n",
       "      <td>0.836335</td>\n",
       "      <td>0.839697</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.849765</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.845853</td>\n",
       "      <td>0.831637</td>\n",
       "      <td>0.848081</td>\n",
       "      <td>0.844707</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.843505</td>\n",
       "      <td>0.845853</td>\n",
       "      <td>0.832551</td>\n",
       "      <td>0.823023</td>\n",
       "      <td>0.840251</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.008324</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_tfidf__max_df param_tfidf__ngram_range param_tfidf__use_idf  \\\n",
       "0                 0.01                   (1, 1)                 True   \n",
       "1                 0.01                   (1, 1)                False   \n",
       "2                 0.01                   (1, 2)                 True   \n",
       "3                 0.01                   (1, 2)                False   \n",
       "4                 0.05                   (1, 1)                 True   \n",
       "5                 0.05                   (1, 1)                False   \n",
       "6                 0.05                   (1, 2)                 True   \n",
       "7                 0.05                   (1, 2)                False   \n",
       "8                  0.1                   (1, 1)                 True   \n",
       "9                  0.1                   (1, 1)                False   \n",
       "10                 0.1                   (1, 2)                 True   \n",
       "11                 0.1                   (1, 2)                False   \n",
       "12                 0.2                   (1, 1)                 True   \n",
       "13                 0.2                   (1, 1)                False   \n",
       "14                 0.2                   (1, 2)                 True   \n",
       "15                 0.2                   (1, 2)                False   \n",
       "16                 1.0                   (1, 1)                 True   \n",
       "17                 1.0                   (1, 1)                False   \n",
       "18                 1.0                   (1, 2)                 True   \n",
       "19                 1.0                   (1, 2)                False   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.822379           0.805947           0.805164   \n",
       "1            0.819249           0.805164           0.794210   \n",
       "2            0.840376           0.830203           0.837246   \n",
       "3            0.838811           0.820814           0.823944   \n",
       "4            0.837246           0.838028           0.834898   \n",
       "5            0.831768           0.827074           0.826291   \n",
       "6            0.837246           0.843505           0.832551   \n",
       "7            0.825509           0.833333           0.820814   \n",
       "8            0.836463           0.836463           0.834898   \n",
       "9            0.833333           0.827074           0.825509   \n",
       "10           0.840376           0.840376           0.830203   \n",
       "11           0.826291           0.831768           0.824726   \n",
       "12           0.854460           0.851330           0.850548   \n",
       "13           0.841158           0.848200           0.844288   \n",
       "14           0.852113           0.848983           0.845853   \n",
       "15           0.843505           0.844288           0.832551   \n",
       "16           0.856025           0.852895           0.853678   \n",
       "17           0.845070           0.845070           0.841941   \n",
       "18           0.849765           0.848200           0.845853   \n",
       "19           0.843505           0.845853           0.832551   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.779953           0.810493         0.804787        0.013858   \n",
       "1            0.768990           0.796398         0.796802        0.016457   \n",
       "2            0.797181           0.847298         0.830461        0.017524   \n",
       "3            0.790916           0.829287         0.820754        0.016120   \n",
       "4            0.818324           0.835552         0.832810        0.007330   \n",
       "5            0.810493           0.824589         0.824043        0.007181   \n",
       "6            0.819890           0.847298         0.836098        0.009559   \n",
       "7            0.811276           0.830854         0.824357        0.007846   \n",
       "8            0.820673           0.839468         0.833593        0.006627   \n",
       "9            0.811276           0.821457         0.823730        0.007305   \n",
       "10           0.818324           0.844166         0.834689        0.009407   \n",
       "11           0.810493           0.825372         0.823730        0.007072   \n",
       "12           0.837118           0.851214         0.848934        0.006061   \n",
       "13           0.830854           0.840251         0.840950        0.005765   \n",
       "14           0.833986           0.847298         0.845647        0.006192   \n",
       "15           0.825372           0.840251         0.837193        0.007222   \n",
       "16           0.835552           0.851214         0.849873        0.007326   \n",
       "17           0.830070           0.836335         0.839697        0.005775   \n",
       "18           0.831637           0.848081         0.844707        0.006653   \n",
       "19           0.823023           0.840251         0.837037        0.008324   \n",
       "\n",
       "    rank_test_score  \n",
       "0                19  \n",
       "1                20  \n",
       "2                13  \n",
       "3                18  \n",
       "4                12  \n",
       "5                15  \n",
       "6                 9  \n",
       "7                14  \n",
       "8                11  \n",
       "9                17  \n",
       "10               10  \n",
       "11               16  \n",
       "12                2  \n",
       "13                5  \n",
       "14                3  \n",
       "15                7  \n",
       "16                1  \n",
       "17                6  \n",
       "18                4  \n",
       "19                8  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(grid.cv_results_).filter(regex='(param_.*)|(.*test_score)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6bf3b9",
   "metadata": {},
   "source": [
    "# Multi-Models\n",
    "\n",
    "Classification of text documents using sparse features https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "> The example design a benchmark for reporting all model training and testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f134639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe= Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        ((tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(max_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=50),\n",
    "         \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a9a38",
   "metadata": {},
   "source": [
    "# (In-Complete) Multiple models with different parameters\n",
    "- different models in pipeline for gridsearchv https://stackoverflow.com/questions/50265993/alternate-different-models-in-pipeline-for-gridsearchcv\n",
    "- plot grid search stats. https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8401b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "models = {\n",
    "#     'tfidf': TfidfVectorizer(max_df=0.05, stop_words=stopwords),\n",
    "    'RF': RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LR': LogisticRegression(),\n",
    "    'NB': GaussianNB(),\n",
    "    'GB': GradientBoostingClassifier(),\n",
    "    'SVM': svm.SVC()\n",
    "}\n",
    "\n",
    "params = {\n",
    "#     'tfidf': {\n",
    "#         'ngram_range': [(1, 1), (1, 2)]\n",
    "#     },\n",
    "    'RF':{ \n",
    "            \"n_estimators\": [100, 200, 500, 1000],\n",
    "            \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "            \"bootstrap\": [True],\n",
    "            \"criterion\": ['gini', 'entropy'],\n",
    "            \"oob_score\": [True, False]\n",
    "            },\n",
    "    'KNN': {\n",
    "        'n_neighbors': range(3, 15),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute']\n",
    "        },\n",
    "    'LR': {\n",
    "        'solver': ['newton-cg', 'sag', 'lbfgs'],\n",
    "        'multi_class': ['ovr', 'multinomial']\n",
    "        }  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d08f242",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.05,\n                                 stop_words=['?', '、', '。', '“', '”', '《', '》',\n                                             '！', '，', '：', '；', '？', '人民',\n                                             '末##末', '啊', '阿', '哎', '哎呀', '哎喲',\n                                             '唉', '我', '我們', '按', '按照', '依照',\n                                             '吧', '吧噠', '把', '罷了', '被', ...])),\n                ('clf',\n                 TfidfVectorizer(max_df=0.05,\n                                 stop_words=['?', '、', '。', '“', '”', '《', '》',\n                                             '！', '，', '：', '；', '？', '人民',\n                                             '末##末', '啊', '阿', '哎', '哎呀', '哎喲',\n                                             '唉', '我', '我們', '按', '按照', '依照',\n                                             '吧', '吧噠', '把', '罷了', '被', ...]))]) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4792893a4a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ])\n\u001b[1;32m      6\u001b[0m     \u001b[0mgscv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best parameters are: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 % estimator)\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('tfidf',\n                 TfidfVectorizer(max_df=0.05,\n                                 stop_words=['?', '、', '。', '“', '”', '《', '》',\n                                             '！', '，', '：', '；', '？', '人民',\n                                             '末##末', '啊', '阿', '哎', '哎呀', '哎喲',\n                                             '唉', '我', '我們', '按', '按照', '依照',\n                                             '吧', '吧噠', '把', '罷了', '被', ...])),\n                ('clf',\n                 TfidfVectorizer(max_df=0.05,\n                                 stop_words=['?', '、', '。', '“', '”', '《', '》',\n                                             '！', '，', '：', '；', '？', '人民',\n                                             '末##末', '啊', '阿', '哎', '哎呀', '哎喲',\n                                             '唉', '我', '我們', '按', '按照', '依照',\n                                             '吧', '吧噠', '把', '罷了', '被', ...]))]) does not."
     ]
    }
   ],
   "source": [
    "for name in models.keys():\n",
    "    text_clf = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_df=0.05, stop_words=stopwords)),\n",
    "        ('clf', models[name])\n",
    "    ])\n",
    "    gscv = GridSearchCV(text_clf, param_grid=params[name], cv=5)\n",
    "    gscv.fit(X_train, y_train)\n",
    "    print(\"best parameters are: {}\".format(gscv.best_estimator_))\n",
    "    y_pred = gscv.predict(X_test)\n",
    "    print(grid.best_score_)\n",
    "    print(accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
